{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Me\n",
    "\n",
    "The one debate that has taken over my thoughts over the past year, continously changing my stance, is that of the Efficient Market Hypothesis, or random walk. Questions like \"if the market is in fact governed by our rational and irrational impulses, and presents inefficiencies, then how come hedge funds and investors have a 50% success rate, the same as a coin flip telling us when to buy or sell? Is it true that \"blind-folded monkey throwing darts\" would do as well a Wall Street analyst?\" are contrasted by opposing questions like \"if small illiquid markets are proven to be consistintly beat, how is it possible that no inefficiency that can be exploited are found in larger markets?\". It seems as the market has to be vulnerable to arbitrage, insider trading and event-driven HFT (unless we assume strong efficient markets), but no one is able to get edge over the market. \n",
    "\n",
    "The following notebook is a few paths taken in the direction to find any set of rules, discovered or not, that govern how markets move. Above all, though, it's a where I, and one perhaps, anyone reading this can just experiment with new technologies and strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with Market Sentiment (V 1.0)\n",
    " With the rise of computing power and of publically available data, a hot-topic today is \"Sentiment\". One of the most basic trading strategies is that of analysing how a group of people (ie the world) feel about a certain topic (ie a company) to predict how the market will move. It makes sense, exchange markets, as the name itself entails, are governed by supply and demand, stocks' prices are determined by what investors as a whole feel the price should be. When investing we are not betting on the future intrinsic value of the stock (if such exists), but on what others think it will be worth.\n",
    " Therefore, the most obvious way to predict how people will feel in the future is analysing how they feel now. That is Sentiment Analysis. Unfortunately, I am not the only one to have thought this way, in fact, the following graph shows the search frequency of the query \"Sentiment analysis\", according to Google.\n",
    "<img src=\"sentimentSearchHistory.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Twitter Volume\n",
    "To start off we will create a model that can search real time for keywords or hashtags on Twitter to predict predefined stock prices. We will plot the frequency (tweets per minute) of a keyword next to the graph of a predifined stock index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import necessary libaries\n",
    "Tweepy is Twitter's API that allows us to convieniently search their database of tweets according to author, keywords, popularity and other handy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Do necessary authentications\n",
    "The following is what allows us to access the twitter API.\n",
    "TODO: Hide secret keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'gXtUH5IaLialLO71v1QZhs7rF'\n",
    "consumer_secret= 'A4tHrkqZ6D96dCGwNLOuRBd0juO8qOpaaP89oUmoPrZNxcWNBj' # Secret\n",
    "access_token='1027935030360850432-RLhwU4GVbCtrE3cu2PyxfTFBIoVOx1'\n",
    "access_token_secret='wK62J222AcJso04D6Vpuge5kQbzlW4NJofGM1ncuHFtri' # Secret\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Search the tweets\n",
    "Now we search for the tweets we want to retrieve. In this demo we will use the keyword \"Elon Musk\" to predict Tesla's Stock price (TSLA).\n",
    "Twitter limits the amount of tweets we can receive 'per search' to 100. We must therefore concatenate multiple search to get a statistically significant amount of data. Tweepy, though, also limits the number of requests you can do every 15 minutes (<180). \n",
    "Another problem is tweepy doesn't make all tweets available but only the ones of the last 6-9 days.\n",
    "\n",
    "We are gonna store the tweets we fetched in `tweets_to_analyse`\n",
    "\n",
    "**This takes a few minutes...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "[{'message': 'Rate limit exceeded', 'code': 88}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-26ef214fec3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mresult_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"recent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         lang = \"en\")\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtweets_to_analyse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_rate_limit_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: [{'message': 'Rate limit exceeded', 'code': 88}]"
     ]
    }
   ],
   "source": [
    "searchQuery = \"LIKE 'elonmusk'\" # Keyword\n",
    "tweets_to_analyse = [] # All the tweets \n",
    "\n",
    "number_of_requests = 30\n",
    "max_id = float('inf') # This is because the first time we search we dont want any max ID\n",
    "\n",
    "for i in range(number_of_requests):\n",
    "    tweets = api.search(q=searchQuery, count=100, #tweets for this search\n",
    "                        result_type = \"recent\",\n",
    "                        max_id = max_id,\n",
    "                        lang = \"en\")\n",
    "    tweets_to_analyse.extend(tweets)\n",
    "    \n",
    "    if(len(tweets) < 100): #if no more tweets are available\n",
    "        break\n",
    "        \n",
    "    max_id = tweets[99].id #get the id of the last item\n",
    "\n",
    "print(\"We have retrieved %s tweets\" % len(tweets_to_analyse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Elon Musk'\n",
    "max_tweets = 1000\n",
    "tweets_to_analyse = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plotting Volume of tweets\n",
    "We now want to visualise this data. The idea is that if I have 100 tweets, I get the first and last's timestamps and set my x-axis (time) to have those as extremes. After that i get the tweets that are multiples of 10 and measure the frequency of tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "number_of_buckets = 10 #This determines how precise we want our graph to be (how many points to have)\n",
    "\n",
    "#get the time delta between our first and last registered (timedelta object)\n",
    "most_recent_timestamp = tweets_to_analyse[0].created_at # this is our 0. We count back in minutes from here\n",
    "oldest_timestamp = tweets_to_analyse[len(tweets_to_analyse)-1].created_at\n",
    "total_data_time = most_recent_timestamp - oldest_timestamp #this is the datetime.timedelta that rapresents the amount of time we recorded\n",
    "\n",
    "delta_in_seconds = -total_data_time.total_seconds() / number_of_buckets #this is negative\n",
    "\n",
    "#generate the x values\n",
    "x_data = [0] * number_of_buckets\n",
    "print(total_data_time)\n",
    "for i in range(number_of_buckets):\n",
    "    x_data[i] = (delta_in_seconds * (i+1))/60\n",
    "\n",
    "#initialize array for y values\n",
    "tweet_volume = [0] * number_of_buckets\n",
    "#calculate y values\n",
    "current_bucket_number = 0\n",
    "for tweet in tweets_to_analyse:\n",
    "    #print(\"bucket number:\", current_bucket_number)\n",
    "    #print(\"is\", (tweet.created_at - most_recent_timestamp).total_seconds(), \"smaller than\", ((current_bucket_number+1) * delta_in_seconds))\n",
    "    #print((tweet.created_at - most_recent_timestamp).total_seconds() < (current_bucket_number+1) * delta_in_seconds)\n",
    "    tweet_volume[current_bucket_number] += 1\n",
    "    if ((tweet.created_at - most_recent_timestamp).total_seconds() < ((current_bucket_number+1) * delta_in_seconds)):\n",
    "        current_bucket_number += 1\n",
    "\n",
    "print(tweet_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, tweet_volume, label = \"tweet volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in_minutes_elapsed_since_oldest = int((datetime.now() - oldest_timestamp).total_seconds()/60)\n",
    "\n",
    "###############GET STOCK DATA#################################\n",
    "#Get data from stock api\n",
    "response = requests.get(\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=TSLA&interval=5min&apikey=H882E1XRM4DIVG86&outputsize=full\") #size to full for full day\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "# set up the minute increment\n",
    "x_values = [-x for x in list(range(0, int(total_data_time.total_seconds()/60), 5))] # these are timestamps that count back from the present to the oldest tweet in increments of 1 minute\n",
    "prices = []\n",
    "\n",
    "counter = 0\n",
    "for data in json_data['Time Series (5min)']:\n",
    "    if (counter >=int(total_data_time.total_seconds()/60/5)): #Only get the last x minutes worth of data\n",
    "        break\n",
    "    data = float((json_data['Time Series (5min)'][data][\"1. open\"]))\n",
    "    prices.append(data)\n",
    "    counter += 1\n",
    "\n",
    "print(len(prices), len(x_values), total_data_time)\n",
    "print(x_values)\n",
    "#plt.plot(x_values, prices, label = \"TSLA price\")\n",
    "plt.plot(x_data, tweet_volume, label = \"Tweets about Musk\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_values, prices, label = \"TSLA price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there positive correlation between tweet sentiment and stock price?\n",
    "We are going to use textblob library to perform sentiment analysis on each of the tweets that will score the tweet on a scale from -1 to +1 where -1 is a negative sentiment, 0 and objective statement or a statement without sentiment and +1 a positive statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-b71c1165a1a3>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-b71c1165a1a3>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    ***\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "x_values = [] # the timestamps of the tweets, counting backwards from present\n",
    "y_values = [] # the sentiment of the tweets [-1; +1]\n",
    "for tweet in tweets_to_analyse:\n",
    "    tweet_date = tweet.created_at\n",
    "    rel_date = tweet_date - most_recent_timestamp\n",
    "    x_values.append(rel_date.total_seconds())\n",
    "    y_values.append(TextBlob(tweet.text).sentiment.polarity)\n",
    "    \n",
    "plt.plot(x_values, y_values, label = \"Tweet sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a spider to scrape off other media outlets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and possible improvements\n",
    "While from a technical standpoint this was an interesting approach, for a financial point of view no ground breaking discoveries were made. While I was expecting to find correlation between tweet volume and/or sentiment compared to stock prices, I was expecting to be unexploitable as I thought much more advanced sentiment analysis tools and faster computers would be able to buy and sell accordingly, in less than a few a milliseconds. I seem to have found no noticeable correlation, one option would to be to apply **linear regression** or more complicated *machine learning* models in order find patterns.\n",
    "It would also be easier to analyse historical data rather than real time data, maybe analysing some big events that lead to sores or plummits of a stock, just to see if even there any patterns can be noticed; and once having developed a model, then testing it in a larger unrpredictable \"common-day\" market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
